# RAG Evaluation Framework Configuration

# General settings
general:
  project_name: "RAG Evaluation Dataset"
  version: "0.1.0"
  description: "Comprehensive evaluation dataset for Retrieval-Augmented Generation systems"
  authors:
    - "Your Organization"
  domain: "General"  # Options: General, Medical, Legal, Technical, etc.
  date_created: "2025-02-25"

# Evaluation metrics configuration
metrics:
  retrieval:
    use_precision_at_k: true
    k_values: [1, 3, 5, 10]
    use_recall_at_k: true
    use_mrr: true
    use_ndcg: true
    relevance_threshold: 0.7  # Minimum relevance score to consider a document relevant

  context:
    use_context_precision: true
    use_context_recall: true
    use_context_conciseness: true
    use_context_coverage: true
    
  answer:
    use_factual_correctness: true
    use_comprehensiveness: true
    use_conciseness: true
    use_rouge: true
    rouge_types: ["rouge1", "rouge2", "rougeL"]
    use_bleu: true
    use_bertscore: true
    bertscore_model: "microsoft/deberta-xlarge-mnli"
    
  robustness:
    use_query_variation: true
    number_of_variations: 3
    use_edge_case_handling: true
    use_impossible_detection: true

# Human evaluation configuration
human_evaluation:
  enabled: true
  number_of_evaluators: 3
  agreement_threshold: 0.7  # Minimum agreement required between evaluators
  rubrics:
    context_relevance: "evaluation/rubrics/context_relevance.md"
    answer_quality: "evaluation/rubrics/answer_quality.md"
  annotation_tool: "custom"  # Options: custom, labelstudio, prodigy, mturk
  sampling:
    method: "stratified"  # Options: random, stratified
    percentage: 0.2  # Percentage of dataset to evaluate manually
    min_samples: 100  # Minimum number of samples to evaluate

# Question generation configuration
question_generation:
  taxonomy_file: "question_sets/taxonomy/question_taxonomy.md"
  distribution_file: "question_sets/taxonomy/distribution.yaml"
  seed_questions:
    min_per_category: 10
    total_min: 100
  automatic_generation:
    enabled: true
    methods:
      - "template_based"
      - "paraphrasing"
      - "llm_generated"
    review_percentage: 1.0  # Percentage of auto-generated questions to manually review

# Target systems for evaluation
target_systems:
  - name: "baseline_rag"
    description: "Basic BM25 + GPT retrieval system"
    api_endpoint: "http://localhost:8000/query"
    request_format: "json"
    authentication:
      type: "bearer"
      token: "${BASELINE_RAG_TOKEN}"
  - name: "advanced_rag"
    description: "Dense retrieval + reranking system"
    api_endpoint: "http://localhost:8001/query"
    request_format: "json"
    authentication:
      type: "api_key"
      key: "${ADVANCED_RAG_TOKEN}"
      header_name: "X-API-Key"

# Results and reporting
results:
  output_directory: "evaluation/results"
  formats:
    - "json"
    - "csv"
    - "html"
  visualization:
    enabled: true
    charts:
      - type: "bar"
        metrics: ["precision@5", "recall@5", "mrr"]
      - type: "radar"
        metrics: ["factual_correctness", "comprehensiveness", "conciseness"]
      - type: "confusion_matrix"
        for: "impossible_question_detection"
  report_template: "evaluation/templates/report_template.md"
  include_examples:
    best_performing: 5
    worst_performing: 5
    average_performing: 5
    
# Advanced settings
advanced:
  parallelism: 4  # Number of parallel evaluation processes
  timeout: 30  # Seconds to wait for a response from target system
  retry:
    attempts: 3
    backoff_factor: 2
  caching:
    enabled: true
    directory: "evaluation/cache"
    ttl: 86400  # Cache time-to-live in seconds (24 hours)
  logging:
    level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR
    file: "evaluation/logs/evaluation.log"
    